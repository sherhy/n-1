Is Technology Applied Science?
The idea that technology is applied science is now centuries old. In the
early seventeenth century, Francis Bacon and René Descartes both promoted
scientific research by claiming that it would produce useful technology. In
the twentieth century this view was importantly championed by Vannevar
Bush, one of the architects of the science policy pursued by the United
States after World War II: “Basic research . . . creates the fund from which
the practical applications of knowledge must be drawn. New products and
new processes do not appear full-grown. They are founded on new principles
and new conceptions, which in turn are painstakingly developed by
research in the purest realms of science. . . . Today, it is truer than ever that
basic research is the pacemaker of technological progress.” The basic-applied
research connection is part of a “linear model” that traces innovation from
basic research to applied research to development and finally to production.
That linear model developed over the first two-thirds of the twentieth
century, as a rhetorical tool used by scientists, management experts, and
economists (Godin 2006).
However, accounts of artifacts and technologies show that scientific
knowledge plays little direct role in the development of even many state of
the art technologies. Historians and other theorists of technology have argued
that there are technological knowledge traditions that are independent
of science, and that to understand the artifacts one needs to understand
them.
Because of its large investment in basic research, in the mid-1960s the
US Department of Defense conducted audits to discover how valuable
that research was. Project Hindsight was a study of key events leading
to the development of 20 weapons systems. It classified 91% of the key
events as technological, 8.7% as applied science, and 0.3% as basic science.
